{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUkkxJFWDNnbY/pYHeXQ6N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bohdan-at-Kulinich/Bohdan-at-Kulinich/blob/main/Class_Regress.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21R32fXyL17-"
      },
      "outputs": [],
      "source": [
        "# Loading the imdb dataset  \n",
        "\n",
        " # keep only the top 10,000 most frequently occuring words in the training data\n",
        " # rare words are discarded to work with the data of manageable size\n",
        " # the whole ds contains 85,585 unique words, some occuring in a single sample\n",
        " # which can't be meaninfully used in classification\n",
        "\n",
        "from tensorflow.keras.datasets import imdb\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000) \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # train_data and test_data are lists of reviews\n",
        " # each review is a list of word indices\n",
        " # train_labes and test_labels are lists of 0 (negative) and 1 (positive)\n",
        "\n",
        "train_data[0] \n",
        "train_labels[0] \n"
      ],
      "metadata": {
        "id": "ObCDx_fhPPYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the max value of the word index:\n",
        "# find the max val for each seq and then extract the max from the final list\n",
        "\n",
        "\n",
        "max([max(sequence) for sequence in train_data]) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-isndePPlwH",
        "outputId": "226d2141-bcc7-41f7-832d-2ac23dd7d631"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9999"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# decoding reviews back to words:\n",
        "\n",
        "# create a dictionary mapping words to an integer index\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "# reverse the dictionary, mapping indices to words\n",
        "reverse_word_index = dict (\n",
        "    [(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "# indices are offset by 3 because 0, 1, 2 are reserved for 'padding', 'start of sequence', and 'unknown'\n",
        "decoded_review = \" \".join(\n",
        "    [reverse_word_index.get(i-3, \"?\") for i in train_data[1]])\n",
        "\n"
      ],
      "metadata": {
        "id": "9hndKorbQRA_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_index)"
      ],
      "metadata": {
        "id": "WVLxp5BRSWT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(reverse_word_index)"
      ],
      "metadata": {
        "id": "Rhye8LUPSZxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(decoded_review)"
      ],
      "metadata": {
        "id": "u9X967khSoWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing the data "
      ],
      "metadata": {
        "id": "Jlq8ejpVUkih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we need to transform the lists of integers into tensors:\n",
        "# 1) pad the lists to make them of the same length and turn them into an integer tensor of shape (samples, max_length)\n",
        "# start the model with the Embedding layer\n",
        "# 2) multi-hot encode the lists to turn them into vectors of 0, 1. \n",
        "# use the Dense layer "
      ],
      "metadata": {
        "id": "2TttK0x3S17P"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding the integer sequences via multi-hot encoding \n",
        "\n",
        "import numpy as np \n",
        "def vectroize_sequences(sequences, dimension=10000):\n",
        "  # create an all-zero matrix\n",
        "  results = np.zeros((len(sequences), dimension))\n",
        "  # set specific indices of results matrix to 1\n",
        "  for i, sequence in enumerate(sequences): # label each sequence in the data with an integer\n",
        "    for j in sequence:  # go over each index in the sequence list  \n",
        "      results[i, j] = 1\n",
        "  return results \n",
        "\n",
        "# vectorize the training and test data \n",
        "x_train = vectroize_sequences(train_data)\n",
        "x_test  = vectroize_sequences(test_data) \n",
        "\n",
        "# vectorize the labels\n",
        "y_train = np.asarray(train_labels).astype(\"float32\")\n",
        "y_test = np.asarray(test_labels).astype(\"float32\")"
      ],
      "metadata": {
        "id": "ay1SdHhBV5zS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the model "
      ],
      "metadata": {
        "id": "H_OjHeTVZ1vk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the input data is vectors, the labels are scalars (0, 1)\n",
        "# such kind of situations is handled well with a plain stack of densely connected (Dense) layers with relu activation \n",
        "\n",
        "# Architecture: three-layer model \n",
        "# two intermediate layers with 16 units each\n",
        "# third layer to output the scalar predictions \n",
        "\n",
        "from tensorflow import keras \n",
        "from tensorflow.keras import layers \n",
        "\n",
        "# each Dense layer with a relu activation implements the chain of tensor operations:\n",
        "# output = relu(dot(input, W) + b)\n",
        "# with 16 units the weight matrix W will have shape (input_dimension, 16)\n",
        "# relu (rectified linear unit) is meant to zero out negative values\n",
        "# sigmoid squashes arbitrary values into the [0, 1] interval, outputing something that can be interpreted as probability \n",
        "model = keras.Sequential([ \n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "nWXbW7xUZ4F7"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DxJaUO35cAfx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}